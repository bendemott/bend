# coding: utf-8
"""
This module contains a PyLucene filters that filter based upon hyphen
characters.
"""

__author__ = "Caleb"
__version__ = "0.2"
__status__ = "Prototype"

__all__ = ['HyphenBoundaryFilter', 'HyphenCollapseFilter', 'HyphenToShingleFilter']

import re
import sys

import lucene

def main(argv):
	import argparse
	
	parser = argparse.ArgumentParser(prog=argv[0])
	parser.add_argument('string', metavar="string", help="The string to tokenize.")
	parser.add_argument('-v', '--verbose', action='store_true', help="Print verbose output.")
	args = parser.parse_args(argv[1:])
	
	string = args.string
	verbose = args.verbose
	
	lucene.initVM()
	
	tokens = lucene.WhitespaceTokenizer(lucene.Version.LUCENE_CURRENT, lucene.StringReader(string))
	tee = lucene.TeeSinkTokenFilter(tokens)
	sinks = [tee.newSinkTokenStream() for i in xrange(7 if verbose else 3)]
	tee.consumeAllTokens()
	
	if verbose:
		print "Tokens"
		print "------"
		tokens = sinks.pop()
		term = tokens.addAttribute(lucene.TermAttribute.class_)
		while tokens.incrementToken():
			print "[%s]" % term.term(),
		print "\n"
	
	print "HyphenBoundaryFilter"
	print "--------------------"
	if verbose:
		tokens = HyphenBoundaryFilter(sinks.pop())
		term = tokens.addAttribute(lucene.TermAttribute.class_)
		while tokens.incrementToken():
			print "[%s]" % term.term(),
		print "\n  REPLACE"
	tokens = HyphenBoundaryFilter(sinks.pop(), replace=True)
	term = tokens.addAttribute(lucene.TermAttribute.class_)
	while tokens.incrementToken():
		print "[%s]" % term.term(),
	print "\n"
	
	print "HyphenCollapseFilter"
	print "--------------------"
	if verbose:
		tokens = HyphenCollapseFilter(sinks.pop())
		term = tokens.addAttribute(lucene.TermAttribute.class_)
		while tokens.incrementToken():
			print "[%s]" % term.term(),
		print "\n  REPLACE"
	tokens = HyphenCollapseFilter(sinks.pop(), replace=True)
	term = tokens.addAttribute(lucene.TermAttribute.class_)
	while tokens.incrementToken():
		print "[%s]" % term.term(),
	print "\n"
	
	print "HyphenToShingleFilter"
	print "---------------------"
	if verbose:
		tokens = HyphenToShingleFilter(sinks.pop())
		term = tokens.addAttribute(lucene.TermAttribute.class_)
		while tokens.incrementToken():
			print "[%s]" % term.term(),
		print "\n  REPLACE"
	tokens = HyphenToShingleFilter(sinks.pop(), replace=True)
	term = tokens.addAttribute(lucene.TermAttribute.class_)
	while tokens.incrementToken():
		print "[%s]" % term.term(),
	print
	
	return 0


class HyphenBoundaryFilter(lucene.PythonTokenFilter):
	"""
	The ``HyphenBoundaryFilter`` class is a PyLucene Filter that splits
	words on hyphens.
	
	Instance Attribute:
	
	*offset* (``lucene.OffsetAttribute``) is used for accessing the offset
	of the token in the original un-tokenized string.
	
	*pos* (``lucene.PositionIncrementAttribute``) is used for accessing
	the position of a token relative to the previous token.
	
	*term* (``lucene.TermAttribute``) is used for accessing the value of a
	token.
	"""
	
	def __init__(self, in_stream, replace=None):
		"""
		Initializes the ``WordToNumberFilter`` instance.
		
		*in_stream* (``lucene.PythonTokenStream``) is the token input stream.
		
		*replace* (``bool``) is whether tokens created by this filter should
		replace the encountered tokens (``True``), or if they should be
		combined (i.e., appended after) the encountered tokens. Default is
		``False``.
		"""
		lucene.PythonTokenFilter.__init__(self, in_stream)
		term_attr = self.term = self.addAttribute(lucene.TermAttribute.class_)
		pos_attr = self.pos = self.addAttribute(lucene.PositionIncrementAttribute.class_)
		off_attr = self.offset = self.addAttribute(lucene.OffsetAttribute.class_)
		
		# Get tokens.
		tokens = []
		while in_stream.incrementToken():
			tokens.append((
				term_attr.term(),
				pos_attr.getPositionIncrement(),
				off_attr.startOffset(),
				off_attr.endOffset()
			))
			
		# Filter tokens.
		tokens = self.filter(tokens, replace)
		
		# Setup token iterator.
		self.iter = iter(tokens)
	
	def filter(self, tokens, replace):
		"""
		Filters the tokens.
		
		*tokens* (``list``) the tokens (``str``).
		
		*replace* (``bool``) is whether tokens created by this filter should
		replace the encountered tokens (``True``), or if they should be
		combined (i.e., appended after) the encountered tokens. Default is
		``False``.
		
		Returns the filtered (``list``) tokens (``str``).
		"""
		found = set(t[0] for t in tokens) if not replace else set()
		final = []
		for token in tokens:
			if not replace:
				# Since we are not supposed to replace tokens, append each token
				# before it is consumed.
				final.append(token)
			# Split on hyphens.
			val = token[0]
			pos = token[1] if replace else 0
			off_start = token[2]
			off_end = token[3]
			is_source = off_end - off_start == len(val)
			slice_start = 0
			for slice in val.split('-'):
				slice_end = slice_start + len(slice)
				if slice and slice not in found:
					# Append split tokens.
					found.add(slice)
					if is_source:
						final.append((slice, pos, off_start + slice_start, off_start + slice_end))
					else:
						final.append((slice, pos, off_start, off_end))
					pos = 0
				slice_start = slice_end + 1
		return final
	
	def incrementToken(self):
		"""
		This method sets an internal buffer with the current token each time
		it is called. NOTE: This method must be implemented by
		``lucene.PythonTokenFilter`` subclasses.
		
		Returns ``True`` if more tokens are available; otherwise, ``False``.
		"""
		try:
			val, pos, off_start, off_end = next(self.iter)
			self.term.setTermBuffer(val)
			self.pos.setPositionIncrement(pos)
			self.offset.setOffset(off_start, off_end)
		except StopIteration:
			return False
		return True


class HyphenCollapseFilter(lucene.PythonTokenFilter):
	"""
	The ``HyphenCollapseFilter`` class is a PyLucene Filter that removes
	hyphens from words.
	
	Instance Attribute:
	
	*offset* (``lucene.OffsetAttribute``) is used for accessing the offset
	of the token in the original un-tokenized string.
	
	*pos* (``lucene.PositionIncrementAttribute``) is used for accessing
	the position of a token relative to the previous token.
	
	*term* (``lucene.TermAttribute``) is used for accessing the value of a
	token.
	"""
	
	def __init__(self, in_stream, replace=None):
		"""
		Initializes the ``WordToNumberFilter`` instance.
		
		*in_stream* (``lucene.PythonTokenStream``) is the token input stream.
		
		*replace* (``bool``) is whether tokens created by this filter should
		replace the encountered tokens (``True``), or if they should be
		combined (i.e., appended after) the encountered tokens. Default is
		``False``.
		"""
		lucene.PythonTokenFilter.__init__(self, in_stream)
		term_attr = self.term = self.addAttribute(lucene.TermAttribute.class_)
		pos_attr = self.pos = self.addAttribute(lucene.PositionIncrementAttribute.class_)
		off_attr = self.offset = self.addAttribute(lucene.OffsetAttribute.class_)
		
		# Get tokens.
		tokens = []
		while in_stream.incrementToken():
			tokens.append((
				term_attr.term(),
				pos_attr.getPositionIncrement(),
				off_attr.startOffset(),
				off_attr.endOffset()
			))
			
		# Filter tokens.
		tokens = self.tokens = self.filter(tokens, replace)
		
		# Setup token iterator.
		self.iter = iter(tokens)
	
	def filter(self, tokens, replace):
		"""
		Filters the tokens.
		
		*tokens* (``list``) the tokens (``str``).
		
		*replace* (``bool``) is whether tokens created by this filter should
		replace the encountered tokens (``True``), or if they should be
		combined (i.e., appended after) the encountered tokens. Default is
		``False``.
		
		Returns the filtered (``list``) tokens (``str``).
		"""
		found = set(t[0] for t in tokens) if not replace else set()
		final = []
		for token in tokens:
			if not replace:
				# Since we are not supposed to replace tokens, append each token
				# before it is consumed.
				final.append(token)
			# Collapse hyphens.
			coll = token[0].replace('-', '')
			if coll and coll not in found:
				found.add(coll)
				final.append((coll, token[1] if replace else 0, token[1], token[2]))
		return final
	
	def incrementToken(self):
		"""
		This method sets an internal buffer with the current token each time
		it is called. NOTE: This method must be implemented in a
		``lucene.PythonTokenFilter``.
		
		Returns ``True`` if more tokens are available; otherwise, ``False``.
		"""
		try:
			val, pos, off_start, off_end = next(self.iter)
			self.term.setTermBuffer(val)
			self.pos.setPositionIncrement(pos)
			self.offset.setOffset(off_start, off_end)
		except StopIteration:
			return False
		return True
	
	
class HyphenToShingleFilter(lucene.PythonTokenFilter):
	"""
	The ``HyphenToShingleFilter`` class is a PyLucene Filter that converts
	hyphens to shingles.
	
	Class Attributes:
		
	*hyphen_chars* (``re.RegexObject``) matches hyphen characters: '-'.
	
	Instance Attribute:
	
	*offset* (``lucene.OffsetAttribute``) is used for accessing the offset
	of the token in the original un-tokenized string.
	
	*pos* (``lucene.PositionIncrementAttribute``) is used for accessing
	the position of a token relative to the previous token.
	
	*term* (``lucene.TermAttribute``) is used for accessing the value of a
	token.
	"""
	
	hyphen_chars = re.compile(r"[-]+")
	
	def __init__(self, in_stream, replace=None):
		"""
		Initializes the ``WordToNumberFilter`` instance.
		
		*in_stream* (``lucene.PythonTokenStream``) is the token input stream.
		
		*replace* (``bool``) is whether tokens created by this filter should
		replace the encountered tokens (``True``), or if they should be
		combined (i.e., appended after) the encountered tokens. Default is
		``False``.
		"""
		lucene.PythonTokenFilter.__init__(self, in_stream)
		term_attr = self.term = self.addAttribute(lucene.TermAttribute.class_)
		pos_attr = self.pos = self.addAttribute(lucene.PositionIncrementAttribute.class_)
		off_attr = self.offset = self.addAttribute(lucene.OffsetAttribute.class_)
		
		# Get tokens.
		tokens = []
		while in_stream.incrementToken():
			tokens.append((
				term_attr.term(),
				pos_attr.getPositionIncrement(),
				off_attr.startOffset(),
				off_attr.endOffset()
			))
			
		# Filter tokens.
		tokens = self.filter(tokens, replace)
		
		# Setup token iterator.
		self.iter = iter(tokens)
	
	def filter(self, tokens, replace):
		"""
		Filters the tokens.
		
		*tokens* (``list``) the tokens (``str``).
		
		*replace* (``bool``) is whether tokens created by this filter should
		replace the encountered tokens (``True``), or if they should be
		combined (i.e., appended after) the encountered tokens. Default is
		``False``.
		
		Returns the filtered (``list``) tokens (``str``).
		"""
		hyphen_chars = self.hyphen_chars
		found = set(t[0] for t in tokens) if not replace else set()
		final = []
		for token in tokens:
			if not replace:
				# Since we are not supposed to replace tokens, append each token
				# before it is consumed.
				final.append(token)
			# Shingle hyphens.
			shingle = hyphen_chars.sub(" ", token[0]).strip()
			if shingle and shingle not in found:
				found.add(shingle)
				final.append((shingle, token[1] if replace else 0, token[2], token[3]))
		return final
	
	def incrementToken(self):
		"""
		This method sets an internal buffer with the current token each time
		it is called. NOTE: This method must be implemented in a
		``lucene.PythonTokenFilter``.
		
		Returns ``True`` if more tokens are available; otherwise, ``False``.
		"""
		try:
			val, pos, off_start, off_end = next(self.iter)
			self.term.setTermBuffer(val)
			self.pos.setPositionIncrement(pos)
			self.offset.setOffset(off_start, off_end)
		except StopIteration:
			return False
		return True
	

if __name__ == '__main__':
	sys.exit(main(sys.argv))
