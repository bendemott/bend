'''
Lammetization may be superior to porter-stemming algorithms...

http://stackoverflow.com/questions/771918/how-do-i-do-word-stemming-or-lemmatization


The WordNet Lemmatizer uses the WordNet Database to lookup lemmas. 
Lemmas differ from stems in that a lemma is a canonical form of the word, 
while a stem may not be a real word.

You must install nltk with the "wordnet" plugin to use this module!
Note the the NLTK module itself takes a few seconds to load - so you should load
it as early as possible, and reload the module as infrequent as possible.
'''



import sys
from lucene import initVM, PythonTokenFilter, StandardTokenizer, TermAttribute, PositionIncrementAttribute, OffsetAttribute, StandardTokenizer, StringReader, Version, TokenStream, PositionIncrementAttribute, OffsetAttribute
try:
	from nltk.stem.wordnet import WordNetLemmatizer
except:
	raise ImportError('''no module named 'nltk' - nltk.stem.wordnet is needed ... The nltk download package 'wordnet' is also needed for this module to work!
	Installation Instructions:
	---------------------------
	1.)  Install NLTK if it's not already
	sudo apt-get install python-nltk
	# From a Python Terminal
	
	2.) Download the NLTK "wordnet" library
	>>> import nltk
	>>> nltk.download()
	# (a menu is shown in command-line mode)
	# Press 'd' for the download menu
	# Type 'wordnet' and press Enter to download the wordnet nltk package
	Identifier> wordnet
	# The default download location is /home/{user}/nltk_data
	# copy to /usr/share/nltk_data (and adjust permissions) when download is complete
	
	3.) Test the download
	 >>> import nltk.stem.wordnet
	''')

class LemmatizationFilter(PythonTokenFilter):
	'''
	This is a custom TokenFilter implemented in python for PyLucene
	breaks apart string into multiple english-words 
	website = "web site"
	ongoing = "on going"
	'''
	
	def __init__(self, inStream, replace=False):
		'''
		Args:
			inStream[TokenStream] An object that wraps tokens and features an incrementToken() method
			replace[bool=False] By default output tokens are combined with input tokens, setting this to true replaces input tokens with only output tokens. 
		'''
		assert isinstance(inStream, TokenStream)
		PythonTokenFilter.__init__(self, inStream)
		
		self.term = self.addAttribute(TermAttribute.class_)
		self.pos = self.addAttribute(PositionIncrementAttribute.class_)
		self.offset = self.addAttribute(OffsetAttribute.class_)
		self.inStream = inStream
		self.replace = replace
		
		#This needs to be the last step in the init method
		self.tokens = self._lemmetize(self.inStream)
		self.iterator = iter(self.tokens)

	def incrementToken(self):
		'''
		This method must be implemented for a "TokenStream" / "TokenFilter"
		This method sets an internal class buffer with the current token each 
		time it's called.
		
		The current token text can be retrieved from the inherited method
		by calling:
		stream = WordBoundaryFilter(tokens)
		term = stream.addAttribute(TermAttribute.class_)
		term.term()
		'''
		try:
			val, pos, off_start, off_end = next(self.iterator)
			self.term.setTermBuffer(val)
			self.pos.setPositionIncrement(pos)
			self.offset.setOffset(off_start, off_end)
			return True;
		except StopIteration:
			return False
			
	def reset(self):
		self.iterator = iter(self.tokens)

        
	def _lemmetize(self, inStream):
		"""
		Lammetize words to their logical root.
		
		Lammetization preserves the stem-word in such a way that it still
		remains a valid (english) word unlike other stemming algorithms
		like porter-stemmer.
		"""
		replace = self.replace
		term_attr = self.term
		pos_attr = self.pos
		off_attr = self.offset
		
		tokens = []
		while inStream.incrementToken():
			tokens.append((
				term_attr.term(),
				pos_attr.getPositionIncrement(),
				off_attr.startOffset(),
				off_attr.endOffset()
			))
		
		found = set(t[0] for t in tokens) if not replace else set()
		lmtzr = WordNetLemmatizer()
		roots = []
		for token in tokens:
			if not replace:
				# Since we are not supposed to replace tokens, append each token
				# before it is consumed.
				roots.append(token)
			val = token[0]
			root = lmtzr.lemmatize(val)  #Lammetize the word to a 'root' word
			if root == val and any([root.endswith('ing'), root.endswith('ed'), root.endswith('es')]):
				#If the word is unchanged by default lemetize - try it as a verb.
				root = lmtzr.lemmatize(val, 'v')
			if root not in found:
				found.add(root)
				roots.append((root, token[1] if replace else 0, token[2], token[3]))
		return roots
							
				
			
def main(argv):
	'''
	First we have to build a Ternary Search Tree of Common Words
	'''
	flush = sys.stdout.flush
	
	print "\nInitializing Lucene... ",; flush()
	initVM()
	print "Done"

	text = argv[1]
	print "\nLemmatizationFilter: "
	print "-----------------------"
	tokens = StandardTokenizer(Version.LUCENE_CURRENT, StringReader(text))
	lfilter = LemmatizationFilter(tokens)
	
	term = lfilter.addAttribute(TermAttribute.class_)
	print "  ",
	while lfilter.incrementToken():
		print "[%s]" % term.term(),
	
	
if __name__ == '__main__':
	sys.exit(main(sys.argv))

