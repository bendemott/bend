__author__ = "Ben DeMott"
__date__ = "March 14 2012"

from lucene import initVM, PythonTokenFilter, StandardTokenizer, TermAttribute, PositionIncrementAttribute, OffsetAttribute, StandardTokenizer, StringReader, Version, TokenStream
import re
import sys

'''
NOTE: Old method that used

self.pattern = re.compile('([A-Z][A-Z][a-z])|([a-z][A-Z])')

ntokens = []
for t in self.tokens:
	nt = self._uncamelcase(t)
	if(not replace and t not in nt):
		#if we are to combine tokens, and this token isn't identical to the existing token add it
		ntokens.append(t)
	ntokens.extend(nt)

def _uncamelcase(self, string):
	
	#self.pattern = 
	"""Adds spaces to a camel case string.  Failure to space out string returns the original string.
	>>> space_out_camel_case('DMLSServicesOtherBSTextLLC')
	'DMLS Services Other BS Text LLC'
	"""
	try:
		string = self.pattern.sub(lambda m: m.group()[:1] + " " + m.group()[1:], string)
		return string.split(" ")
	except:
		raise
		return string
'''

class CaseChangeFilter(PythonTokenFilter):
	'''
	This is a custom TokenFilter implemented in python.
	Splits token strings on 'CaseChanges'
	'''
	
	pattern = re.compile(r".+?(?:[A-Z](?=[A-Z][a-z])|[a-z](?=[A-Z])|$)")
	
	#TODO this needs some work with this type of string "HELLOWorld" (probably should ignore this) also should ignored "HeLLo" 
	def __init__(self, inStream, replace=False):
		'''
		Args:
			inStream[TokenStream] Input Token Strings
			replace[bool=False] By default output tokens are combined with input tokens, setting this to true replaces input tokens with only output tokens. 
		'''
		assert isinstance(inStream, TokenStream)
		PythonTokenFilter.__init__(self, inStream)
		term_attr = self.term = self.addAttribute(TermAttribute.class_)
		pos_attr = self.pos = self.addAttribute(PositionIncrementAttribute.class_)
		off_attr = self.offset = self.addAttribute(OffsetAttribute.class_)
		self.tokens = []
		while inStream.incrementToken():
			self.tokens.append((
				term_attr.term(),
				pos_attr.getPositionIncrement(),
				off_attr.startOffset(),
				off_attr.endOffset()
			))
			
		ntokens = self._casechange(self.tokens, replace)
		
		#It's important are returned in their original order, that's why this
		#logic is using a list
		self.tokens = iter(ntokens)

	def incrementToken(self):
		'''
		This method must be implemented for a "TokenStream" / "TokenFilter"
		This method sets an internal class buffer with the current token each 
		time it's called.
		'''
		try:
			val, pos, off_start, off_end = next(self.tokens)
			self.term.setTermBuffer(val)
			self.pos.setPositionIncrement(pos)
			self.offset.setOffset(off_start, off_end)
			return True;
		except StopIteration:
			return False
		
	def _casechange(self, tokens, replace):
		"""
		Splits tokens on case change.
		
		*tokens* (``list``) the tokens (``str``).
		
		*replace* (``bool``) is whether tokens created by this filter should
		replace the encountered tokens (``True``), or if they should be
		combined (i.e., appended after) the encountered tokens. Default is
		``False``.
		
		Returns the filtered (``list``) tokens (``str``).
		"""
		pattern = self.pattern
		found = set(t[0] for t in tokens) if not replace else set()
		final = []
		for token in tokens:
			if not replace:
				# Since we are not supposed to replace tokens, append each token
				# before it is consumed.
				final.append(token)
			# Uncamelcase token
			val = token[0]
			pos = token[1] if replace else 0
			off_start = token[2]
			off_end = token[3]
			is_source = off_end - off_start == len(val)
			for match in pattern.finditer(val):
				slice = match.group(0)
				if slice and slice not in found:
					found.add(slice)
					if is_source:
						final.append((slice, pos, off_start + match.start(), off_start + match.end()))
					else:
						final.append((slice, pos, off_start, off_end))
					pos = 0
		return final			
			
def main(argv):
	'''
	First we have to build a Ternary Search Tree of Common Words
	'''
	flush = sys.stdout.flush
	
	print "\nInitializing Lucene... ",; flush()
	initVM()
	print "Done"

	text = argv[1]
	print "\nCaseChangeFilter: "
	print "-----------------------"
	tokens = StandardTokenizer(Version.LUCENE_CURRENT, StringReader(text))
	cfilter = CaseChangeFilter(tokens)
	
	term = cfilter.addAttribute(TermAttribute.class_)
	print "  ",
	while cfilter.incrementToken():
		print "[%s]" % term.term(),
	
	
if __name__ == '__main__':
	sys.exit(main(sys.argv))

