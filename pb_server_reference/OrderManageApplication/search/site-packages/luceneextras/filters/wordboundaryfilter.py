'''
This will implement a word-boundary filter compatible with lucene that splits
words based on their text boundary.
'''

import sys
import tst
from lucene import *

class WordBoundaryFilter(PythonTokenFilter):
	'''
	This is a custom TokenFilter implemented in python for PyLucene
	breaks apart string into multiple english-words 
	website = "web site"
	ongoing = "on going"
	'''
	
	def __init__(self, inStream, tst, replace=False):
		'''
		Args:
			inStream[TokenStream] An object that wraps tokens and features an incrementToken() method
			tst [tst.TST] A ternary search tree object from the python 'tst' module containing the dictionary to use for word-evaluation
			replace[bool=False] By default output tokens are combined with input tokens, setting this to true replaces input tokens with only output tokens. 
		'''
		assert isinstance(inStream, StandardTokenizer)
		PythonTokenFilter.__init__(self, inStream)
		self.term = self.addAttribute(TermAttribute.class_)
		self.inStream = inStream
		self.tst = tst
		self.replace=replace

		#This needs to be the last step in the init method
		self.tokens = self._splitwords(self.inStream)
		self.iterator = iter(self.tokens)


	def incrementToken(self):
		'''
		This method must be implemented for a "TokenStream" / "TokenFilter"
		This method sets an internal class buffer with the current token each 
		time it's called.
		
		The current token text can be retrieved from the inherited method
		by calling:
		stream = WordBoundaryFilter(tokens)
		term = stream.addAttribute(TermAttribute.class_)
		term.term()
		'''
		try:
			self.term.setTermBuffer(self.iterator.next())  #Set the current item in the buffer
			return True;
		except StopIteration:
			return False
			
	def reset(self):
		self.iterator = iter(self.tokens)

        
	def _splitwords(self, inStream):
		"""
		Split words into multiple tokens (returns a list)
		
		We have to find the LONGEST form of a valid word.  If there is junk at
		the end of a word don't consider it as part of the second word
		
		SEARCH THE TST:
			tree.walk(None, tst.DictAction(), search)
		"""
		tokens = []
		while self.inStream.incrementToken():
			tokens.append(self.term.term())
			
		words = []
		for ts in tokens: #Iterate over the Token Strings (ts)
			if len(ts) < 4:
				words.append(ts) #Add it back into the Filter's Tokens
				continue
				
			wordtext = ts
			ts = ts.lower()
				
			matches = [] #word matches each row = (word-start-idx, word-length, internal-match-str)
			match_words = []
			cwrd = None #current word
			cwrd_start = 0
			lwrd = None #last word
			
			'''
			Iterate until you have a word match... once there is a word match
			append to matches the start, and end index of the match.
			As we iterate if lwrd is set, check lwrd + cwrd for matches
			If lwrd + cwrd match, then we have a new match, combine them into
			lwrd, adjust the last match because we found a longer word and
			continue
			'''
			for i, c in enumerate(ts):
				if(cwrd is None):
					cwrd_start = i
					cwrd = ''
				cwrd = cwrd + c
				
				if(len(match_words)):
					#This should result in a search of LEFT-TO-RIGHT of words already encountered.
					# ['al', 'pin'] + 'e' = 'alpine'  
					# This is how the algorithm finds the largest word... in
					# the example above al, and pin are words so they've been
					# placed into the matches.
					for ii, _ in enumerate(match_words):
						if(not cwrd):
							break
						raw_search = "".join(match_words[ii:])+cwrd
						search = raw_search
						#search = self._singularize(raw_search)
						results = self.tst.walk(None, tst.DictAction(), search)
						if not results:
							continue
							
						keys = results.keys()
						keys.sort(key=len, reverse=True) #Sort the keys, longest first!
						for m in keys:
							# Both strings should be identical length, and the matched string should start at index ZERO in it's matching
							if len(search) == len(m) and results[m][0] == 0:
								#print "MATCH2:", m, raw_search
								# If its a full-word match
								l = list(matches[ii])
								pops = len(match_words) - ii
								#print "POPPING: %s for %s from %s ... %s" % (pops, search, match_words, matches)
								for iii in range(0, pops):
									#print "III", iii
									match_words.pop() #Remove items from the end of the match_words that were used for the matching
									matches.pop()
								#print "Matches: ", matches
								l[1] = (i - l[0]) + 1
								l[2] = m
								match_words.append(raw_search)
								matches.append(l)
								cwrd = None #reset the cwrd
								break
				
				if(cwrd and len(cwrd) > 1):
					#search = self._singularize(cwrd)  #XXX
					search = cwrd
					#print "SEARCH1:", search
					results = self.tst.walk(None, tst.DictAction(), search)
					if len(results):
						keys = results.keys()
						keys.sort(key=len, reverse=True) #Sort the keys, longest first!
						for m in keys:
							# Both strings should be identical length, and the matched string should start at index ZERO in it's matching
							if len(search) == len(m) and results[m][0] == 0:
								#print "MATCH: ", m, cwrd
								# If its a full-word match
								match_words.append(cwrd)
								matches.append( [cwrd_start, (i-cwrd_start)+1, m] )
								cwrd_start = i+1
								cwrd = None #reset the cwrd
								break
							
			#Append the matches on Token String
			newtokens = []
			if not self.replace:
				#If we are not to replace terms combine the original term with the new terms
				newtokens.append(wordtext)
			
			for m in matches:
				word = wordtext[m[0]: m[0]+m[1]]  #Token String [start, length]
				#Don't duplicate identical terms
				if word not in newtokens and word not in words:
					newtokens.append(word)
			words.extend(newtokens)
				
		#Combine Tokens, and words, in the original order, ignoring duplicates
		
		return words
				
			
def main(argv):
	'''
	First we have to build a Ternary Search Tree of Common Words
	'''
	flush = sys.stdout.flush
	
	print "\nInitializing Lucene... ",; flush()
	initVM()
	print "Done"
	
	wordfile = "common_english_dictionary.txt"
	print "\nOpening %s... " % wordfile,; flush()
	fp = open(wordfile)
	print "Done"
	
	twoletter = "common_english_2letter.txt"
	print "\nOpening %s... " % twoletter,; flush()
	fp2 = open(twoletter)
	print "Done"
	
	print "\nBuilding Ternary Search Tree... ",; flush()
	tree = tst.TST()
	print "Done"

	twoletter = [line.strip().lower() for line in fp2]

	for i, word in enumerate(fp):
		word = word.strip().lower()
		if ' ' in word:
			continue
		if word.startswith('-') or word.startswith('#') or word.endswith('-'):
			continue
		if not word:
			continue
		if len(word) <= 2 and word not in twoletter:
			continue

		tree[word] = i

	text = argv[1]
	print "\nWordBoundaryFilter:"
	print "---------------------"
	tokens = StandardTokenizer(Version.LUCENE_CURRENT, StringReader(text))
	lfilter = WordBoundaryFilter(tokens, tree)
	
	term = lfilter.addAttribute(TermAttribute.class_)
	print "  ",
	while lfilter.incrementToken():
		print "[%s]" % term.term(),
	
	
if __name__ == '__main__':
	sys.exit(main(sys.argv))

