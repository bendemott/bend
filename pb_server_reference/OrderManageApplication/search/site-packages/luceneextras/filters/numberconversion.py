# coding: utf-8
"""
This module contains PyLucene filters that convert between numbers and
their numerical words.
"""
from __future__ import division

__author__ = "Caleb"
__version__ = "0.2"
__status__ = "Prototype"

__all__ = ['NumberToWordFilter', 'WordToNumberFilter']

import re
import sys

import lucene

def main(argv):
	import argparse
	
	parser = argparse.ArgumentParser(prog=argv[0])
	parser.add_argument('string', metavar="string", help="The string to tokenize.")
	parser.add_argument('-v', '--verbose', action='store_true', help="Print verbose output.")
	args = parser.parse_args(argv[1:])
	
	string = args.string
	verbose = args.verbose
	
	lucene.initVM()
	
	tokens = lucene.WhitespaceTokenizer(lucene.Version.LUCENE_CURRENT, lucene.StringReader(string))
	tee = lucene.TeeSinkTokenFilter(tokens)
	sinks = [tee.newSinkTokenStream() for i in xrange(5 if verbose else 2)]
	tee.consumeAllTokens()
	
	if verbose:
		print "Tokens"
		print "------"
		tokens = sinks.pop()
		term = tokens.addAttribute(lucene.TermAttribute.class_)
		while tokens.incrementToken():
			print "[%s]" % term.term(),
		print "\n"
	
	print "NumberToWordFilter"
	print "------------------"
	if verbose:
		tokens = NumberToWordFilter(sinks.pop())
		term = tokens.addAttribute(lucene.TermAttribute.class_)
		while tokens.incrementToken():
			print "[%s]" % term.term(),
		print "\n  REPLACE"
	tokens = NumberToWordFilter(sinks.pop(), replace=True)
	term = tokens.addAttribute(lucene.TermAttribute.class_)
	while tokens.incrementToken():
		print "[%s]" % term.term(),
	print "\n"
	
	print "WordToNumberFilter"
	print "------------------"
	if verbose:
		tokens = WordToNumberFilter(sinks.pop())
		term = tokens.addAttribute(lucene.TermAttribute.class_)
		while tokens.incrementToken():
			print "[%s]" % term.term(),
		print "\n  REPLACE"
	tokens = WordToNumberFilter(sinks.pop(), replace=True)
	term = tokens.addAttribute(lucene.TermAttribute.class_)
	while tokens.incrementToken():
		print "[%s]" % term.term(),
	print
	
	return 0


class NumberToWordFilter(lucene.PythonTokenFilter):
	"""
	The ``NumberToWordFilter`` is a PyLucene Token Filter that converts 
	numbers to numerical words.
	
	Class Attributes:
		
	*num_to_word* (``dict``) maps numbers (``int``) to their corresponding
	numerical word (``str``).
	
	Instance Attribute:
	
	*offset* (``lucene.OffsetAttribute``) is used for accessing the offset
	of the token in the original un-tokenized string.
	
	*pos* (``lucene.PositionIncrementAttribute``) is used for accessing
	the position of a token relative to the previous token.
	
	*term* (``lucene.TermAttribute``) is used for accessing the value of a
	token.
	"""
	
	num_to_word = {
		0: "zero",
		1: "one",
		2: "two",
		3: "three",
		4: "four",
		5: "five",
		6: "six",
		7: "seven",
		8: "eight",
		9: "nine",
		10: "ten",
		11: "eleven",
		12: "twelve",
		13: "thirteen",
		14: "fourteen",
		15: "fifteen",
		16: "sixteen",
		17: "seventeen",
		18: "eighteen",
		19: "nineteen",
		20: "twenty",
		30: "thirty",
		40: "forty",
		50: "fifty",
		60: "sixty",
		70: "seventy",
		80: "eighty",
		90: "ninety",
		100: "hundred",
		1000: "thousand",
		1000000: "million"
	}
	
	is_num = re.compile(r"^\d+$")
	
	def __init__(self, in_stream, replace=None):
		"""
		Initializes the ``NumberToWordFilter`` instance.
		
		*in_stream* (``lucene.PythonTokenStream``) is the token input stream.
		
		*replace* (``bool``) is whether tokens created by this filter should
		replace the encountered tokens (``True``), or if they should be
		combined (i.e., appended after) the encountered tokens. Default is
		``False``.
		"""
		lucene.PythonTokenFilter.__init__(self, in_stream)
		term_attr = self.term = self.addAttribute(lucene.TermAttribute.class_)
		pos_attr = self.pos = self.addAttribute(lucene.PositionIncrementAttribute.class_)
		off_attr = self.offset = self.addAttribute(lucene.OffsetAttribute.class_)
		
		# Get tokens.
		tokens = []
		while in_stream.incrementToken():
			tokens.append((
				term_attr.term(),
				pos_attr.getPositionIncrement(),
				off_attr.startOffset(),
				off_attr.endOffset()
			))
			
		# Filter tokens.
		tokens = self.filter(tokens, replace)
		
		# Setup token iterator.
		self.iter = iter(tokens)
		
	def filter(self, tokens, replace):
		"""
		Filters the tokens.
		
		*tokens* (``list``) the tokens (``str``).
		
		*replace* (``bool``) is whether tokens created by this filter should
		replace the encountered tokens (``True``), or if they should be
		combined (i.e., appended after) the encountered tokens. Default is
		``False``.
		
		Returns the filtered (``list``) tokens (``str``).
		"""
		num_to_word = self.num_to_word
		is_num = self.is_num
		found = set(t[0] for t in tokens)
		final = []
		for token in tokens:
			val = token[0]
			
			if not replace:
				# Since we are not supposed to replace tokens, append each token
				# before it is consumed.
				final.append(token)
			
			if not is_num.match(val):
				# Since this token is not a number, append it and continue to
				# the next one.
				if replace:
					found.add(val)
					final.append(token)
				continue
			
			pos = token[1] if replace else 0
			off_start = token[2]
			off_end = token[3]
			
			# Reverse digits from token.
			digits = val.lstrip('0')[::-1]
			if not digits:
				# Since we had zero, append it.
				val = num_to_word[0]
				if val not in found:
					found.add(val)
					final.append((val, pos, off_start, off_end))
				continue
			
			# Group every 3 digits and iterate over grouped digits in reverse
			# so that groups are yielded in the original order and in each
			# group:
			#   0 -> ones
			#   1 -> tens
			#   2 -> hundreds
			groups = [digits[i:i+3] for i in xrange(0, len(digits), 3)][::-1]
			scale = len(groups) - 1
			result = []
			for oth in groups:
				l = len(oth)
				if l == 3 and oth[2] != '0':
					# 2 -> x
					# 1 -> .
					# 0 -> .
					result.append(num_to_word[int(oth[2])])
					result.append(num_to_word[100])
				if l >= 2:
					t = oth[1]
					if t == '1':
						# 1 -> 1
						# 0 -> x
						result.append(num_to_word[int(oth[1::-1])])
					else:
						if t != '0':
							# 1 -> x
							# 0 -> x
							result.append(num_to_word[int(t)*10])
						if oth[0] != '0':
							result.append(num_to_word[int(oth[0])])
				elif oth[0] != '0':
					# 0 -> x
					result.append(num_to_word[int(oth[0])])
			
				# Add scale modifier: million or thousand.
				s = scale
				if s % 2:
					result.append(num_to_word[1000])
				while s >= 2:
					result.append(num_to_word[1000000])
					s -= 2
				scale -= 1
				
			for word in result:
				if word not in found:
					found.add(word)
					final.append((word, pos, off_start, off_end))
					pos = 0
					
		return final
	
	def incrementToken(self):
		"""
		This method sets an internal buffer with the current token each time
		it is called. NOTE: This method must be implemented in a
		``lucene.PythonTokenFilter``.
		
		Returns ``True`` if more tokens are available; otherwise, ``False``.
		"""
		try:
			val, pos, off_start, off_end = next(self.iter)
			self.term.setTermBuffer(val)
			self.pos.setPositionIncrement(pos)
			self.offset.setOffset(off_start, off_end)
		except StopIteration:
			return False
		return True


class WordToNumberFilter(lucene.PythonTokenFilter):
	"""
	The ``WordToNumberFilter`` class is a PyLucene Token Filter that
	converts numerical words to numbers.
	
	Class Attributes:
		
	*word_to_num* (``dict``) maps numerical words (``str``) to their
	corresponding number (``int``).
	
	Instance Attribute:
	
	*offset* (``lucene.OffsetAttribute``) is used for accessing the offset
	of the token in the original un-tokenized string.
	
	*pos* (``lucene.PositionIncrementAttribute``) is used for accessing
	the position of a token relative to the previous token.
	
	*term* (``lucene.TermAttribute``) is used for accessing the value of a
	token.
	"""
	
	word_to_num = {
		"zero": 0,
		"one": 1,
		"two": 2,
		"three": 3,
		"four": 4,
		"five": 5,
		"six": 6,
		"seven": 7,
		"eight": 8,
		"nine": 9,
		"ten": 10,
		"eleven": 11,
		"twelve": 12,
		"thirteen": 13,
		"fourteen": 14,
		"fifteen": 15,
		"sixteen": 16,
		"seventeen": 17,
		"eighteen": 18,
		"nineteen": 19,
		"twenty": 20,
		"thirty": 30,
		"forty": 40,
		"fifty": 50,
		"sixty": 60,
		"seventy": 70,
		"eighty": 80,
		"ninety": 90,
		"hundred": 100,
		"thousand": 1000,
		"million": 1000000
	}
	
	def __init__(self, in_stream, replace=None):
		"""
		Initializes the ``WordToNumberFilter`` instance.
		
		*in_stream* (``lucene.PythonTokenStream``) is the token input stream.
		
		*replace* (``bool``) is whether tokens created by this filter should
		replace the encountered tokens (``True``), or if they should be
		combined (i.e., appended after) the encountered tokens. Default is
		``False``.
		"""
		lucene.PythonTokenFilter.__init__(self, in_stream)
		term_attr = self.term = self.addAttribute(lucene.TermAttribute.class_)
		pos_attr = self.pos = self.addAttribute(lucene.PositionIncrementAttribute.class_)
		off_attr = self.offset = self.addAttribute(lucene.OffsetAttribute.class_)
		
		# Get tokens.
		tokens = []
		while in_stream.incrementToken():
			tokens.append((
				term_attr.term(),
				pos_attr.getPositionIncrement(),
				off_attr.startOffset(),
				off_attr.endOffset()
			))
			
		# Filter tokens.
		tokens = self.filter(tokens, replace)
		
		# Setup token iterator.
		self.iter = iter(tokens)
		
	def filter(self, tokens, replace):
		"""
		Filters the tokens.
		
		*tokens* (``list``) the tokens (``str``).
		
		*replace* (``bool``) is whether tokens created by this filter should
		replace the encountered tokens (``True``), or if they should be
		combined (i.e., appended after) the encountered tokens. Default is
		``False``.
		
		Returns the filtered (``list``) tokens (``str``).
		"""
		word_to_num = self.word_to_num
		found = set(t[0] for t in tokens)
		final = []
		acc = None
		acc_pos = None
		acc_start = None
		acc_end = None
		
		for token in tokens:
			val = token[0]
			if val not in word_to_num:
				# Since this token is not a number, append the accumulated
				# number and then this token.
				if acc is not None:
					acc = str(acc)
					if acc not in found:
						found.add(acc)
						final.append((acc, acc_pos, acc_start, acc_end))
					acc = None
				final.append(token)
				continue
				
			pos = token[1] if replace else 0
			
			num = word_to_num[val]
			if num >= 100:
				# We have a multiplier (e.g., x100) so append the resulting
				# number and clear the accumulator.
				if acc is None:
					acc = str(num)
					acc_pos = pos
					acc_start = token[2]
					acc_end = token[3]
				else:
					acc = str(acc * num)
					acc_end = token[3]
				if acc not in found:
					found.add(acc)
					final.append((acc, acc_pos, acc_start, acc_end))
				acc = None
			elif num >= 10:
				# We have a starting number so append the accumulated number,
				# and set the accumulator to our new number.
				if acc is not None:
					acc = str(acc)
					if acc not in found:
						found.add(acc)
						final.append((acc, acc_pos, acc_start, acc_end))
				acc = num
				acc_pos = pos
				acc_start = token[2]
				acc_end = token[3]
			else: # 0 <= num <= 9
				# We have an ending number so add it to the accumulator if it is
				# a compound number (e.g., 20); otherwise (e.g., 17), append the
				# accumulated number and set the accumulator to our new number.
				if acc >= 20 and not acc % 10: 
					acc += num
					acc_end = token[3]
				else:
					if acc is not None:
						acc = str(acc)
						if acc not in found:
							found.add(acc)
							final.append((acc, acc_pos, acc_start, acc_end))
					acc = num
					acc_pos = pos
					acc_start = token[2]
					acc_end = token[3]
			
			if not replace:
				# Since we are not supposed to replace tokens, append each
				# token.
				final.append(token)
					
		# If we have a number left in the accumulator, append it.
		if acc is not None:	
			acc = str(acc)
			if acc not in found:
				final.append((acc, acc_pos, acc_start, acc_end))
					
		return final
	
	def incrementToken(self):
		"""
		This method sets an internal buffer with the current token each time
		it is called. NOTE: This method must be implemented in a
		``lucene.PythonTokenFilter``.
		
		Returns ``True`` if more tokens are available; otherwise, ``False``.
		"""
		try:
			val, pos, off_start, off_end = next(self.iter)
			self.term.setTermBuffer(val)
			self.pos.setPositionIncrement(pos)
			self.offset.setOffset(off_start, off_end)
		except StopIteration:
			return False
		return True


if __name__ == '__main__':
	sys.exit(main(sys.argv))
